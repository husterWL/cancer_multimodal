{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"8xH3m4AbOxEi"},"outputs":[],"source":["import tables\n","import os,sys\n","import glob\n","import PIL\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from sklearn import model_selection\n","from skimage import io as skio\n","from skimage.io import imread_collection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"YVVtusN6OxEn","lines_to_next_cell":2},"outputs":[],"source":["# filename\n","dataname=\"sclerosis\"\n","patch_size=256 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"yWEwKrGfOxEq"},"outputs":[],"source":["img_dtype = tables.UInt8Atom()  # dtype in which the images will be saved, this indicates that images will be saved as unsigned int 8 bit, i.e., [0,255]\n","filenameAtom = tables.StringAtom(itemsize=255) #create an atom to store the filename of the image, just incase we need it later, "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"20ktL9uzOxEt"},"source":["## Training/ validation data setup "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ok2ILOZ_OxEt"},"source":["Collect all training slides' patches, then divide into training/validation dataset of hdf5 format(pytable). We can also divide dataset at slide level. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"w5F67u0ROxEu","outputId":"d268d6c5-5ff6-4e31-fc98-b4b728aa8866"},"outputs":[{"name":"stdout","output_type":"stream","text":["['D16', 'D17', 'D18', 'D19']\n"]}],"source":["# read all training slide ids\n","IDs = os.listdir(\"./masks_patch\")\n","print(IDs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"6uJIEoC0OxEx","outputId":"2c637af9-ac96-4e2e-d9c8-f8eda0f1ff09","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["D16\n","D17\n","D18\n","D19\n","maskpatch_collection (277, 256, 256)\n","slidepatch_collection (277, 256, 256, 3)\n","namelist (277,)\n"]}],"source":["# collect all patches \n","images = []\n","masks = []\n","names = []\n","\n","for idi in IDs[:]:\n","    print(idi)\n","    # collect mask patches for slide_idi\n","    col = np.array(imread_collection(r\"./masks_patch/{}/*.png\".format(idi)))\n","    l = len(col)\n","    if(l==0): #skip empty folders \n","        continue\n","    masks.append(col[:]) \n","    # collect slide patches\n","    col = np.array(imread_collection(\"./slides_patch/{}/*.png\".format(idi)))\n","    images.append(col[:])\n","    # collect names\n","    name1 = os.listdir(\"./masks_patch/{}\".format(idi))\n","    # make sure names in same order as imread_collection\n","    name2 = np.array(sorted(name1,key=lambda e: int(e.split('_')[2])))\n","    names.append(list(name2[:])) \n","    \n","# concate patches from all slides     \n","col_msk = np.concatenate(masks)\n","col_img = np.concatenate(images)\n","col_name = np.concatenate(names)\n","print(\"maskpatch_collection\",col_msk.shape)\n","print(\"slidepatch_collection\",col_img.shape)\n","print(\"namelist\",col_name.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"s0Q5SHpbOxEz","outputId":"85d7eb21-6736-42bf-ac6d-5c0e071ceb59"},"outputs":[{"name":"stdout","output_type":"stream","text":["num of train patch: 221 \n","num of val patch: 56\n"]}],"source":["# random split training/validation patches at 8:2 ratio\n","train,val = next(iter(model_selection.ShuffleSplit(n_splits=1,test_size=0.2).split(col_msk)))\n","print(\"num of train patch:\",len(train),\"\\nnum of val patch:\",len(val))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"wtBb7iVvOxE1","outputId":"33b612fa-c28f-4db5-d8bf-060543b0582b"},"outputs":[{"name":"stdout","output_type":"stream","text":["img_train (221, 256, 256, 3)\n","msk_train (221, 256, 256)\n","name_train (221,)\n"]}],"source":["# prepare for store in hdf5 file\n","img={}\n","msk={}\n","name={}\n","img[\"train\"] = col_img[train]\n","img[\"val\"] = col_img[val]\n","msk[\"train\"] = col_msk[train]\n","msk[\"val\"] = col_msk[val]\n","name[\"val\"] = col_name[val]\n","name[\"train\"] = col_name[train]\n","print(\"img_train\",img[\"train\"].shape)\n","print(\"msk_train\",msk[\"train\"].shape)\n","print(\"name_train\",name[\"train\"].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"vUS8BdDUOxE4"},"outputs":[],"source":["#setup hdf5 file\n","storage={} #holder for future pytables\n","imgtypes=[\"img\",\"msk\"]\n","patch_size = 256\n","block_shape={} #block shape specifies what we'll be saving into the pytable array, here we assume that masks are 1d and images are 3d\n","block_shape[\"img\"]= np.array((patch_size,patch_size,3))\n","block_shape[\"msk\"]= np.array((patch_size,patch_size)) \n","filters=tables.Filters(complevel=6, complib='zlib') #we can also specify filters, such as compression, to improve storage speed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"3CpSKo-LOxE6","outputId":"d1ecacb5-a549-4cfc-83c7-d3f8b41b1f6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["generate hdf5 file  sclerosis_train.pytable\n","generate hdf5 file  sclerosis_val.pytable\n"]}],"source":["# fill in hdf5 file of training/validtion datasets\n","phases = [\"train\",\"val\"]\n","for phase in phases: #now for each of the phases, we'll loop through the files\n","    \n","    print(\"generate hdf5 file \",dataname+'_'+phase+'.pytable')\n","\n","    hdf5_file = tables.open_file(f\"../data/{dataname}_{phase}.pytable\", mode='w') #open the respective pytable, here we choose to store in training data dir\n","    storage[\"filename\"] = hdf5_file.create_earray(hdf5_file.root, 'filename', filenameAtom, (0,)) #create the array for storage\n","    storage[\"filename\"].append(name[phase]) #add the filename to the storage array\n","    \n","    for imgtype in imgtypes: #for each of the image types, in this case mask and image, we need to create the associated earray\n","        storage[imgtype]= hdf5_file.create_earray(hdf5_file.root, imgtype, img_dtype,  \n","                                                  shape=np.append([0],block_shape[imgtype]), \n","                                                  chunkshape=np.append([1],block_shape[imgtype]),\n","                                                  filters=filters)        \n","        #save the 4D tensor to the table\n","        if(imgtype==\"img\"):\n","            storage[imgtype].append(img[phase])\n","        elif(imgtype==\"msk\"):\n","            storage[imgtype].append(msk[phase])\n","\n","    hdf5_file.close()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"orF4jwT7OxE9"},"source":["## Testing data setup "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AzUK8PisOxE-"},"source":["Same as training data setup, just using testing slides as input with filename \"{dataname}_test\" "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"AcyNv1R2OxE-"},"outputs":[],"source":["images = []\n","masks = []\n","names = []\n","\n","# get testing slides IDs \n","for idi in IDs[:]:\n","    print(idi)\n","    col = np.array(imread_collection(r\"./masks_patch/{}/*.png\".format(idi)))\n","    l = len(col)\n","    if(l==0):\n","        continue\n","    masks.append(col[:])\n","    col = np.array(imread_collection(\"./slides_patch/{}/*.png\".format(idi)))\n","    images.append(col[:])\n","    name1 = os.listdir(\"./masks_patch/{}\".format(idi))\n","    name2 = np.array(sorted(name1,key=lambda e: int(e.split('_')[2])))\n","    names.append(list(name2[:])) \n","    \n","col_msk = np.concatenate(masks)\n","col_img = np.concatenate(images)\n","col_name = np.concatenate(names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"cvtPfFE0OxFF"},"outputs":[],"source":["# setup hdf5 file\n","img={}\n","msk={}\n","name={}\n","img[\"test\"] = col_img[:]\n","msk[\"test\"] = col_msk[:]\n","name[\"test\"] = col_name[:]\n","\n","storage={} \n","imgtypes=[\"img\",\"msk\"]\n","patch_size = 256\n","block_shape={} \n","block_shape[\"img\"]= np.array((patch_size,patch_size,3))\n","block_shape[\"msk\"]= np.array((patch_size,patch_size)) \n","filters=tables.Filters(complevel=6, complib='zlib') \n","\n","phases = [\"test\"] # phase change to testing\n","\n","for phase in phases:\n","    print(phase)\n","\n","    hdf5_file = tables.open_file(f\"E:/PATH FINAL/{dataname}_{phase}.pytable\", mode='w') \n","    storage[\"filename\"] = hdf5_file.create_earray(hdf5_file.root, 'filename', filenameAtom, (0,)) \n","    storage[\"filename\"].append(name[phase]) \n","    print(len(storage[\"filename\"]))\n","    for imgtype in imgtypes: \n","        storage[imgtype]= hdf5_file.create_earray(hdf5_file.root, imgtype, img_dtype,  \n","                                                  shape=np.append([0],block_shape[imgtype]), \n","                                                  chunkshape=np.append([1],block_shape[imgtype]),\n","                                                  filters=filters)        \n","        if(imgtype==\"img\"):\n","            storage[imgtype].append(img[phase])\n","        elif(imgtype==\"msk\"):\n","            storage[imgtype].append(msk[phase])\n","    hdf5_file.close()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TvIuJZwkOxFH","lines_to_next_cell":2},"source":["reference<br>\n","https://github.com/choosehappy/PytorchDigitalPathology/blob/master/segmentation_epistroma_unet/make_hdf5.ipynb\n","http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"step2_make_HDF5.ipynb","provenance":[]},"jupytext":{"formats":"ipynb,py:light"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
